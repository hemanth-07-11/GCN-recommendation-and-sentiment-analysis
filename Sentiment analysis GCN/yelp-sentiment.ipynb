{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## YELP DATASET - Sentiment Analysis Draft","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport re\nimport nltk\nimport string\nfrom collections import Counter\nfrom torch.utils.data import DataLoader\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-12T09:18:39.653867Z","iopub.execute_input":"2023-04-12T09:18:39.654358Z","iopub.status.idle":"2023-04-12T09:18:42.535073Z","shell.execute_reply.started":"2023-04-12T09:18:39.654227Z","shell.execute_reply":"2023-04-12T09:18:42.533971Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"cols = [\"sentiment\", \"review\"]\ntrain_orig = pd.read_csv(\"../input/yelp-review-dataset/yelp_review_polarity_csv/train.csv\", names=cols)\ntest = pd.read_csv(\"../input/yelp-review-dataset/yelp_review_polarity_csv/test.csv\", names=cols)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:18:42.536941Z","iopub.execute_input":"2023-04-12T09:18:42.537616Z","iopub.status.idle":"2023-04-12T09:18:52.817647Z","shell.execute_reply.started":"2023-04-12T09:18:42.537573Z","shell.execute_reply":"2023-04-12T09:18:52.816598Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_ = train_orig.groupby('sentiment',as_index=False).apply(lambda x: x.sample(frac=0.3))\ntrain_ = train_.reset_index( level = 0, drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:18:52.819652Z","iopub.execute_input":"2023-04-12T09:18:52.819977Z","iopub.status.idle":"2023-04-12T09:18:53.059036Z","shell.execute_reply.started":"2023-04-12T09:18:52.819944Z","shell.execute_reply":"2023-04-12T09:18:53.057986Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n    #text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text) \n    return text","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:18:53.060749Z","iopub.execute_input":"2023-04-12T09:18:53.061074Z","iopub.status.idle":"2023-04-12T09:18:53.065321Z","shell.execute_reply.started":"2023-04-12T09:18:53.061042Z","shell.execute_reply":"2023-04-12T09:18:53.064468Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train = train_.copy()\ntrain.review = train.review.apply(clean_text)\ntest.review = test.review.apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:18:53.066553Z","iopub.execute_input":"2023-04-12T09:18:53.067005Z","iopub.status.idle":"2023-04-12T09:18:59.346949Z","shell.execute_reply.started":"2023-04-12T09:18:53.066964Z","shell.execute_reply":"2023-04-12T09:18:59.345985Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(train[\"review\"], train[\"sentiment\"], test_size=0.15, random_state=42, stratify=train[\"sentiment\"])\ntrain = pd.concat([X_train, y_train], axis=1)\nvalid = pd.concat([X_valid, y_valid], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:18:59.348081Z","iopub.execute_input":"2023-04-12T09:18:59.348533Z","iopub.status.idle":"2023-04-12T09:18:59.455937Z","shell.execute_reply.started":"2023-04-12T09:18:59.348502Z","shell.execute_reply":"2023-04-12T09:18:59.454766Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train[\"split\"] = \"train\"\ntest[\"split\"] = \"test\"\nvalid[\"split\"] = \"val\"\nreviews_df = pd.concat([train, test, valid])\nreviews_df.rename(columns={\"sentiment\": \"rating\"}, inplace=True)\nreviews_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:18:59.458104Z","iopub.execute_input":"2023-04-12T09:18:59.458496Z","iopub.status.idle":"2023-04-12T09:18:59.525691Z","shell.execute_reply.started":"2023-04-12T09:18:59.458459Z","shell.execute_reply":"2023-04-12T09:18:59.524414Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                   review  rating  split\n329348  i should have checked yelp before coming here ...       1  train\n510482  when was the last time you heard rock-n-roll b...       2  train\n132069  the service here just was not good .  it seeme...       1  train\n115598  upon walking into the store today ,  i observe...       1  train\n173477  just had lunch there with the homie . \\n\\nthe ...       2  train","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>rating</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>329348</th>\n      <td>i should have checked yelp before coming here ...</td>\n      <td>1</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>510482</th>\n      <td>when was the last time you heard rock-n-roll b...</td>\n      <td>2</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>132069</th>\n      <td>the service here just was not good .  it seeme...</td>\n      <td>1</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>115598</th>\n      <td>upon walking into the store today ,  i observe...</td>\n      <td>1</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>173477</th>\n      <td>just had lunch there with the homie . \\n\\nthe ...</td>\n      <td>2</td>\n      <td>train</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"reviews_df.to_csv(\"./reviews.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:18:59.528478Z","iopub.execute_input":"2023-04-12T09:18:59.528877Z","iopub.status.idle":"2023-04-12T09:19:02.875278Z","shell.execute_reply.started":"2023-04-12T09:18:59.528842Z","shell.execute_reply":"2023-04-12T09:19:02.873987Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Dataset class","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass ReviewDataset(Dataset):\n    def __init__(self, review_df, vectorizer):\n\n        self.review_df = review_df\n        self._vectorizer = vectorizer\n\n        self.train_df = self.review_df[self.review_df.split=='train']\n        self.train_size = len(self.train_df)\n\n        self.val_df = self.review_df[self.review_df.split=='val']\n        self.validation_size = len(self.val_df)\n\n        self.test_df = self.review_df[self.review_df.split=='test']\n        self.test_size = len(self.test_df)\n\n        self._lookup_dict = {'train': (self.train_df, self.train_size),\n                             'val': (self.val_df, self.validation_size),\n                             'test': (self.test_df, self.test_size)}\n\n        self.set_split('train')\n\n    @classmethod\n    def load_dataset_and_make_vectorizer(cls, review_csv):\n\n        review_df = pd.read_csv(review_csv)\n        return cls(review_df, ReviewVectorizer.from_dataframe(review_df))\n\n    def get_vectorizer(self):\n        return self._vectorizer\n\n    def set_split(self, split=\"train\"):\n\n        self._target_split = split\n        self._target_df, self._target_size = self._lookup_dict[split]\n\n    def __len__(self):\n        return self._target_size\n\n    def __getitem__(self, index):\n        row = self._target_df.iloc[index]\n        review_vector = \\\n            self._vectorizer.vectorize(row.review)\n        rating_index = \\\n            self._vectorizer.rating_vocab.lookup_token(row.rating)\n        return {'x_data': review_vector,\n                'y_target': rating_index}\n\n    def get_num_batches(self, batch_size):\n        return len(self) // batch_size","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:19:02.877289Z","iopub.execute_input":"2023-04-12T09:19:02.877648Z","iopub.status.idle":"2023-04-12T09:19:02.890170Z","shell.execute_reply.started":"2023-04-12T09:19:02.877614Z","shell.execute_reply":"2023-04-12T09:19:02.888950Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Vocabulary","metadata":{}},{"cell_type":"code","source":"class Vocabulary(object):\n\n    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n        if token_to_idx is None:\n            token_to_idx = {}\n        self._token_to_idx = token_to_idx\n\n        self._idx_to_token = {idx: token \n                              for token, idx in self._token_to_idx.items()}\n\n        self._add_unk = add_unk\n        self._unk_token = unk_token\n        \n        self.unk_index = -1\n        if add_unk:\n            self.unk_index = self.add_token(unk_token) \n        \n        \n    def to_serializable(self):\n        return {'token_to_idx': self._token_to_idx, \n                'add_unk': self._add_unk, \n                'unk_token': self._unk_token}\n\n    @classmethod\n    def from_serializable(cls, contents):\n        return cls(**contents)\n\n    def add_token(self, token):\n        if token in self._token_to_idx:\n            index = self._token_to_idx[token]\n        else:\n            index = len(self._token_to_idx)\n            self._token_to_idx[token] = index\n            self._idx_to_token[index] = token\n        return index\n\n    def lookup_token(self, token):\n        if self._add_unk:\n            return self._token_to_idx.get(token, self.unk_index)\n        else:\n            return self._token_to_idx[token]\n\n    def lookup_index(self, index):\n        if index not in self._idx_to_token:\n            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n        return self._idx_to_token[index]\n\n    def __str__(self):\n        return \"<Vocabulary(size=%d)>\" % len(self)\n\n    def __len__(self):\n        return len(self._token_to_idx)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:19:02.891742Z","iopub.execute_input":"2023-04-12T09:19:02.892069Z","iopub.status.idle":"2023-04-12T09:19:02.906894Z","shell.execute_reply.started":"2023-04-12T09:19:02.892037Z","shell.execute_reply":"2023-04-12T09:19:02.905819Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Vectorizer","metadata":{"execution":{"iopub.status.busy":"2021-09-09T09:18:02.678535Z","iopub.execute_input":"2021-09-09T09:18:02.678975Z","iopub.status.idle":"2021-09-09T09:18:02.776015Z","shell.execute_reply.started":"2021-09-09T09:18:02.678938Z","shell.execute_reply":"2021-09-09T09:18:02.774153Z"}}},{"cell_type":"code","source":"class ReviewVectorizer(object):\n    def __init__(self, review_vocab, rating_vocab):\n        self.review_vocab = review_vocab\n        self.rating_vocab = rating_vocab\n\n    def vectorize(self, review):\n        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n        \n        for token in review.split(\" \"):\n            if token not in string.punctuation:\n                one_hot[self.review_vocab.lookup_token(token)] = 1\n\n        return one_hot\n\n    @classmethod\n    def from_dataframe(cls, review_df, cutoff=25):\n        review_vocab = Vocabulary(add_unk=True)\n        rating_vocab = Vocabulary(add_unk=False)\n\n        for rating in sorted(set(review_df.rating)):\n            rating_vocab.add_token(rating)\n\n        word_counts = Counter()\n        for review in review_df.review:\n            for word in review.split(\" \"):\n                if word not in string.punctuation:\n                    word_counts[word] += 1\n               \n        for word, count in word_counts.items():\n            if count > cutoff:\n                review_vocab.add_token(word)\n\n        return cls(review_vocab, rating_vocab)\n\n    @classmethod\n    def from_serializable(cls, contents):\n        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n        rating_vocab =  Vocabulary.from_serializable(contents['rating_vocab'])\n\n        return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n\n    def to_serializable(self):\n        return {'review_vocab': self.review_vocab.to_serializable(),\n                'rating_vocab': self.rating_vocab.to_serializable()}","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:19:02.908236Z","iopub.execute_input":"2023-04-12T09:19:02.908654Z","iopub.status.idle":"2023-04-12T09:19:02.923084Z","shell.execute_reply.started":"2023-04-12T09:19:02.908601Z","shell.execute_reply":"2023-04-12T09:19:02.922239Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Dataloader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef generate_batches(dataset, batch_size, shuffle=True,\n                     drop_last=True, device=\"cpu\"):\n    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n                            shuffle=shuffle, drop_last=drop_last)\n\n    for data_dict in dataloader:\n        out_data_dict = {}\n        for name, tensor in data_dict.items():\n            out_data_dict[name] = data_dict[name].to(device)\n        yield out_data_dict","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:19:02.924528Z","iopub.execute_input":"2023-04-12T09:19:02.925126Z","iopub.status.idle":"2023-04-12T09:19:02.938189Z","shell.execute_reply.started":"2023-04-12T09:19:02.925082Z","shell.execute_reply":"2023-04-12T09:19:02.936903Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass ReviewClassifier(nn.Module):\n    def __init__(self, num_features):\n        super(ReviewClassifier, self).__init__()\n        self.fc1 = nn.Linear(in_features=num_features, \n                             out_features=1)\n\n    def forward(self, x_in, apply_sigmoid=False):\n        y_out = self.fc1(x_in).squeeze()\n        if apply_sigmoid:\n            y_out = F.sigmoid(y_out)\n        return y_out","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:19:02.940005Z","iopub.execute_input":"2023-04-12T09:19:02.940376Z","iopub.status.idle":"2023-04-12T09:19:02.949169Z","shell.execute_reply.started":"2023-04-12T09:19:02.940301Z","shell.execute_reply":"2023-04-12T09:19:02.948063Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from argparse import Namespace\n\nargs = Namespace(\n\n    frequency_cutoff=25,\n    model_state_file='model.pth',\n    review_csv='./reviews.csv',\n    save_dir='./',\n    vectorizer_file='vectorizer.json',\n    # No model hyperparameters\n    # Training hyperparameters\n    batch_size=128,\n    early_stopping_criteria=5,\n    learning_rate=0.001,\n    num_epochs=100,\n    seed=1337,\n)\nargs","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:19:02.950518Z","iopub.execute_input":"2023-04-12T09:19:02.950856Z","iopub.status.idle":"2023-04-12T09:19:02.968535Z","shell.execute_reply.started":"2023-04-12T09:19:02.950815Z","shell.execute_reply":"2023-04-12T09:19:02.967202Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Namespace(batch_size=128, early_stopping_criteria=5, frequency_cutoff=25, learning_rate=0.001, model_state_file='model.pth', num_epochs=100, review_csv='./reviews.csv', save_dir='./', seed=1337, vectorizer_file='vectorizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"def compute_accuracy(y_pred, y_target):\n    y_target = y_target.cpu()\n    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()\n    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n    return n_correct / len(y_pred_indices) * 100","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:19:02.969709Z","iopub.execute_input":"2023-04-12T09:19:02.970561Z","iopub.status.idle":"2023-04-12T09:19:02.978109Z","shell.execute_reply.started":"2023-04-12T09:19:02.970511Z","shell.execute_reply":"2023-04-12T09:19:02.977061Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim \n\ndef make_train_state(args):\n    return {'epoch_index': 0,\n            'train_loss': [],\n            'train_acc': [],\n            'val_loss': [],\n            'val_acc': [],\n            'test_loss': -1,\n            'test_acc': -1}\ntrain_state = make_train_state(args)\n\nif not torch.cuda.is_available():\n    args.cuda = False\nargs.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n\ndataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\nvectorizer = dataset.get_vectorizer()\n\nclassifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))\nclassifier = classifier.to(args.device)\n\nloss_func = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:19:02.979458Z","iopub.execute_input":"2023-04-12T09:19:02.979775Z","iopub.status.idle":"2023-04-12T09:19:20.506602Z","shell.execute_reply.started":"2023-04-12T09:19:02.979743Z","shell.execute_reply":"2023-04-12T09:19:20.505438Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"for epoch_index in range(args.num_epochs):\n    train_state['epoch_index'] = epoch_index\n    dataset.set_split('train')\n    batch_generator = generate_batches(dataset, \n                                       batch_size=args.batch_size, \n                                       device=args.device)\n    running_loss = 0.0\n    running_acc = 0.0\n    classifier.train()\n    \n    for batch_index, batch_dict in enumerate(batch_generator):\n\n        optimizer.zero_grad()\n\n        y_pred = classifier(x_in=batch_dict['x_data'].float())\n\n        loss = loss_func(y_pred, batch_dict['y_target'].float())\n        loss_batch = loss.item()\n        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n\n        loss.backward()\n\n        optimizer.step()\n        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n\n    train_state['train_loss'].append(running_loss)\n    train_state['train_acc'].append(running_acc)\n    print(\"Accuracy: {} \\nLoss: {}\".format(running_acc, running_loss))\n\n    dataset.set_split('val')\n    batch_generator = generate_batches(dataset, \n                                       batch_size=args.batch_size, \n                                       device=args.device)\n    running_loss = 0.\n    running_acc = 0.\n    classifier.eval()\n\n    for batch_index, batch_dict in enumerate(batch_generator):\n\n        y_pred = classifier(x_in=batch_dict['x_data'].float())\n        loss = loss_func(y_pred, batch_dict['y_target'].float())\n        loss_batch = loss.item()\n        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n\n    train_state['val_loss'].append(running_loss)\n    train_state['val_acc'].append(running_acc)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:19:20.508311Z","iopub.execute_input":"2023-04-12T09:19:20.508672Z","iopub.status.idle":"2023-04-12T09:59:29.877081Z","shell.execute_reply.started":"2023-04-12T09:19:20.508639Z","shell.execute_reply":"2023-04-12T09:59:29.875034Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Accuracy: 89.17741031390136 \nLoss: 0.3456679702606969\nAccuracy: 92.76135089686103 \nLoss: 0.22152302182308756\nAccuracy: 93.48094170403587 \nLoss: 0.19178128307309386\nAccuracy: 93.92586883408062 \nLoss: 0.17610216936589362\nAccuracy: 94.20053251121068 \nLoss: 0.16571850479985567\nAccuracy: 94.49341367713001 \nLoss: 0.1580053976699377\nAccuracy: 94.65036434977569 \nLoss: 0.15198434374792127\nAccuracy: 94.83674327354271 \nLoss: 0.146935841675029\nAccuracy: 95.01331278026917 \nLoss: 0.14257804758078288\nAccuracy: 95.13663116591913 \nLoss: 0.13891067819611366\nAccuracy: 95.25854820627808 \nLoss: 0.13561707294040748\nAccuracy: 95.3685538116593 \nLoss: 0.13274019763140932\nAccuracy: 95.44913116591918 \nLoss: 0.13010793130828116\nAccuracy: 95.54932735426006 \nLoss: 0.12764659672975504\nAccuracy: 95.63340807174882 \nLoss: 0.12548207038921663\nAccuracy: 95.70838004484297 \nLoss: 0.12344878842143737\nAccuracy: 95.77144058295963 \nLoss: 0.12158011188354731\nAccuracy: 95.81978699551584 \nLoss: 0.11975544942668198\nAccuracy: 95.92769058295976 \nLoss: 0.11813845872010344\nAccuracy: 95.9494114349775 \nLoss: 0.1165684497844211\nAccuracy: 95.99075112107619 \nLoss: 0.11514405317170189\nAccuracy: 96.06782511210763 \nLoss: 0.11378240512825967\nAccuracy: 96.09094730941693 \nLoss: 0.11252976851412545\nAccuracy: 96.13438901345292 \nLoss: 0.11122793425626282\nAccuracy: 96.15821188340797 \nLoss: 0.11008160740070282\nAccuracy: 96.217769058296 \nLoss: 0.10896142135487011\nAccuracy: 96.2338845291481 \nLoss: 0.10792607596235966\nAccuracy: 96.28923766816149 \nLoss: 0.10684672605764184\nAccuracy: 96.336182735426 \nLoss: 0.10583825906229129\nAccuracy: 96.35790358744397 \nLoss: 0.10501239621184867\nAccuracy: 96.41465807174886 \nLoss: 0.10398045945327918\nAccuracy: 96.43007286995497 \nLoss: 0.1032307325189959\nAccuracy: 96.46650784753352 \nLoss: 0.1023728996916202\nAccuracy: 96.46510650224198 \nLoss: 0.10157643019017089\nAccuracy: 96.52746636771302 \nLoss: 0.10075186782520726\nAccuracy: 96.57581278026909 \nLoss: 0.1000802692911283\nAccuracy: 96.57371076233173 \nLoss: 0.09939116969236889\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-d22c41926039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m# the training routine is 5 steps:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-fce955c5aa47>\u001b[0m in \u001b[0;36mgenerate_batches\u001b[0;34m(dataset, batch_size, shuffle, drop_last, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m                             shuffle=shuffle, drop_last=drop_last)\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mout_data_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_fields'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# namedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_fields'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# namedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"dataset.set_split('test')\nbatch_generator = generate_batches(dataset, \n                                   batch_size=args.batch_size, \n                                   device=args.device)\nrunning_loss = 0.\nrunning_acc = 0.\nclassifier.eval()\n\nfor batch_index, batch_dict in enumerate(batch_generator):\n    y_pred = classifier(x_in=batch_dict['x_data'].float())\n    loss = loss_func(y_pred, batch_dict['y_target'].float())\n    loss_batch = loss.item()\n    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n\n    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n    running_acc += (acc_batch - running_acc) / (batch_index + 1)\n\ntrain_state['test_loss'] = running_loss\ntrain_state['test_acc'] = running_acc","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:59:37.668166Z","iopub.execute_input":"2023-04-12T09:59:37.668554Z","iopub.status.idle":"2023-04-12T09:59:52.543106Z","shell.execute_reply.started":"2023-04-12T09:59:37.668522Z","shell.execute_reply":"2023-04-12T09:59:52.542114Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def predict_rating(review, classifier, vectorizer,\n                   decision_threshold=0.5):\n\n    review = preprocess_text(review)\n    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n    result = classifier(vectorized_review.view(1, -1))\n\n    probability_value = F.sigmoid(result).item()\n\n    index =  1\n    if probability_value < decision_threshold:\n        index = 0\n\n    return vectorizer.rating_vocab.lookup_index(index)\n\ntest_review = \"this is a pretty awesome book\"\nprediction = predict_rating(test_review, classifier, vectorizer)\nprint(\"{} -> {}\".format(test_review, prediction)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:59:57.399182Z","iopub.execute_input":"2023-04-12T09:59:57.399569Z","iopub.status.idle":"2023-04-12T09:59:57.407769Z","shell.execute_reply.started":"2023-04-12T09:59:57.399536Z","shell.execute_reply":"2023-04-12T09:59:57.406280Z"},"trusted":true},"execution_count":19,"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-67492dcc9e32>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    print(\"{} -> {}\".format(test_review, prediction)\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"],"ename":"SyntaxError","evalue":"unexpected EOF while parsing (<ipython-input-19-67492dcc9e32>, line 27)","output_type":"error"}]},{"cell_type":"code","source":"fc1_weights = classifier.fc1.weight.detach()[0]\n_, indices = torch.sort(fc1_weights, dim=0, descending=True)\nindices = indices.numpy().tolist()\nprint(\"Influential words in Positive Reviews:\")\nprint(\"--------------------------------------\")\nfor i in range(20):\n    print(vectorizer.review_vocab.lookup_index(indices[i]))","metadata":{"execution":{"iopub.status.busy":"2023-04-12T10:00:07.448920Z","iopub.execute_input":"2023-04-12T10:00:07.449642Z","iopub.status.idle":"2023-04-12T10:00:07.471321Z","shell.execute_reply.started":"2023-04-12T10:00:07.449592Z","shell.execute_reply":"2023-04-12T10:00:07.470185Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Influential words in Positive Reviews:\n--------------------------------------\nfanciest\n\\n\\nwe'll\nsinful\nyummm\n\\n\\nexcellent\ndisappoint\nperfection\n=d\nvos\nhells\ngem\nexceeded\n\\nclean\nd\\u00e9licieux\nhesitate\nftw\nsuper-friendly\ntastiest\nyumm\n\\nawesome\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Influential words in Negative Reviews:\")\nprint(\"--------------------------------------\")\nindices.reverse()\nfor i in range(20):\n    print(vectorizer.review_vocab.lookup_index(indices[i]))","metadata":{"execution":{"iopub.status.busy":"2023-04-12T10:00:11.795643Z","iopub.execute_input":"2023-04-12T10:00:11.796015Z","iopub.status.idle":"2023-04-12T10:00:11.805294Z","shell.execute_reply.started":"2023-04-12T10:00:11.795981Z","shell.execute_reply":"2023-04-12T10:00:11.804164Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Influential words in Negative Reviews:\n--------------------------------------\nripoff\nletzten\nslowest\nfurious\n\\\"meh\nmeh\ncockroach\ninedible\nmarginal\naggravating\nsham\nmediocre\nworst\naweful\n\\n\\nmeh\npoisoning\nunderwhelmed\noverrated\nabsurdly\nunacceptable\n","output_type":"stream"}]}]}